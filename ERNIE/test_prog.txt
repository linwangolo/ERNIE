blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "transpose_26.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_25.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_38.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_5"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_9_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_6_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_14.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_15"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_6_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_12.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_8.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_19"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "dropout_18.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_35.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_59.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_34.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_17.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_16.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "scale_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_21.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_8.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_38.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_20.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_36.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_65.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "pre_encoder_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_7.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_30.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_10.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_34.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_16"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "layer_norm_2.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_43.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_31.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_15.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_19.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_29.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_4_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_11.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_10.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_52.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_28.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_44.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_28.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_14.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_36.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_27.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_13.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_30.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_68.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "scale_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_17.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_16.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_25.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_24.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_13.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_8.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_3_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_24.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_12.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_22.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_63.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_23.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_6.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_9_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_9_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_21.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_15.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_3_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_14"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "tmp_21"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "dropout_10.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_12"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "tmp_11"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "scale_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_9.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_4.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_19.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_64.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_12.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_46.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_33.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_10.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "softmax_with_cross_entropy_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_19.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_9.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_48.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_11.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_12.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_14.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_6.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_18.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "softmax_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_10"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "transpose_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_7_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_64.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_9.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_60.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_4"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_4_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_10_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_10_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_15.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_4.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_5.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_22.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_38.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_6.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_18.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_7.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_72.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_8.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_4.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_25"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_1_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_9.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_2_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_1_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "read_file_0.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 256
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_22.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_46.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_42.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "scale_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_32.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_33.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_7.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_42.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_7"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "scale_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_39.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_28"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "reshape2_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_23.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_31.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "embedding_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_9.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "scale_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_16.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_57.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_5.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_37.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_25.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_50.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_23.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "embedding_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_17.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_61.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_41.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_42.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "read_file_0.tmp_4"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_0_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_8"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_26.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "create_tensor_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: 1
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_21.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "scale_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_22"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "matmul_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_7_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_7_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_64.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_0.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_28.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_42.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_72.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_53.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_23"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "softmax_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_45.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_40.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_6_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_43.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_13.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_15.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_24"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_52.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_46.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_22.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_66.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_47.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_22.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_22.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_48.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_32"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_8_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_53.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_8_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_70.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_33.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_62.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_52.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_39.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_35"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_54.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_6"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "dropout_37.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_54.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "read_file_0.tmp_5"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_55.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_24.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_8_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_7_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_45.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_67.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_62.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_67.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_post_ffn_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_60.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_11.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_44.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_15.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_16.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_34.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_63.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_46.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_46.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_34.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_71.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "accuracy_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "pos_embedding"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 513
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_41.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_32.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "scale_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_47.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "slice_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 1
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_27"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "transpose_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_56.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_24.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_21.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_13"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "transpose_47.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_26"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "transpose_35.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_16.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "softmax_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_24.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_multi_head_att_query_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_9"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_8_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "test_reader_double_buffer"
    type {
      type: READER
      reader {
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: FP32
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 1
          }
          lod_level: 0
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_30"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "dropout_35.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "top_k_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_32.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_19.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_20.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_20.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_30.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_18.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_70.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_31"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "layer_norm_23.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "cls_out_b"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_46.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_11_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_14.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_69.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_73.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 2
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "scale_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_69.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_71.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_36"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "mean_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_10_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_9_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_21.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_39.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_59.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_65.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_27.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "cls_out_w"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 2
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_31.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_26.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "embedding_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_39.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_50.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_43.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_34"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_10_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_58.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_39.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "top_k_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_37.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "accuracy_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: 1
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_72.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_43.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "test_reader_reader"
    type {
      type: READER
      reader {
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: FP32
            dims: -1
            dims: 256
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 1
          }
          lod_level: 0
        }
        lod_tensor {
          tensor {
            data_type: INT64
            dims: -1
            dims: 1
          }
          lod_level: 0
        }
      }
    }
    persistable: true
  }
  vars {
    name: "pooled_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_40.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_29.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_37"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_2_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_19.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_51.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_47.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_55.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_18.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_38.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_28.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_3"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "reshape2_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_73.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 2
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_61.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_58.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_49.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_14.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_57.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_with_cross_entropy_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 2
        }
        lod_level: 0
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_33"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_9_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_13.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_2_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_13.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "tmp_29"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "dropout_20.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_6_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_2_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_10_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_6_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_27.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_20"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "encoder_layer_9_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_17"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_6.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_5_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "scale_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_36.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_5.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_70.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_6.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_5.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_2_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_11_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_10.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_7.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_58.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_43.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_3.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "scale_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "read_file_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 256
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_8_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "stack_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_10.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_17.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_7.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "pooled_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_29.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "scale_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_7_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_16.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_multi_head_att_value_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_1_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_12.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_4.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_47.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_4.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "softmax_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_5.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_value_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_5_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_10_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_4.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_output_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_11.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_18"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_51.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_key_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_multi_head_att_key_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_6_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_4_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "scale_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "pre_encoder_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_10_ffn_fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_56.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_9_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_1_post_ffn_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "matmul_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_20.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_11.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_49.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_0_multi_head_att_query_fc.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "dropout_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "matmul_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_40.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 3072
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "sent_embedding"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 2
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_68.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "read_file_0.tmp_3"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_42.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_3_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "reshape2_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_4_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_1.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_66.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_multi_head_att_output_fc.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_1_ffn_fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "word_embedding"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 18000
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_3_ffn_fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_17.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "reshape2_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_post_att_layer_norm_bias"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "transpose_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "transpose_8.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: -1
          dims: 256
          dims: 12
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_38.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "read_file_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 256
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "transpose_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 12
          dims: 256
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_23.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
      }
    }
    persistable: false
  }
  vars {
    name: "encoder_layer_1_post_att_layer_norm_scale"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 768
        }
      }
    }
    persistable: true
  }
  vars {
    name: "encoder_layer_1_ffn_fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 3072
        }
      }
    }
    persistable: true
  }
  vars {
    name: "layer_norm_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "layer_norm_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -256
        }
      }
    }
    persistable: false
  }
  vars {
    name: "dropout_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 256
          dims: 768
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  ops {
    inputs {
      parameter: "UnderlyingReader"
      arguments: "test_reader_reader"
    }
    outputs {
      parameter: "Out"
      arguments: "test_reader_double_buffer"
    }
    type: "create_double_buffer_reader"
    attrs {
      name: "place"
      type: STRING
      s: "AUTO"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/io.py\", line 955, in __create_unshared_decorated_reader__\n    attrs=attrs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/io.py\", line 1045, in double_buffer\n    \'create_double_buffer_reader\', reader, attrs, name=name)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/io.py\", line 551, in _py_reader\n    double_buffer_reader = double_buffer(reader, name=double_buffer_name)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/io.py\", line 778, in py_reader\n    use_double_buffer=use_double_buffer)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 38, in create_model\n    use_double_buffer=True)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Reader"
      arguments: "test_reader_double_buffer"
    }
    outputs {
      parameter: "Out"
      arguments: "read_file_0.tmp_0"
      arguments: "read_file_0.tmp_1"
      arguments: "read_file_0.tmp_2"
      arguments: "read_file_0.tmp_3"
      arguments: "read_file_0.tmp_4"
      arguments: "read_file_0.tmp_5"
    }
    type: "read"
    attrs {
      name: "throw_eof_exp"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "infer_out"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/io.py\", line 1087, in read_file\n    type=\'read\', inputs={\'Reader\': [reader]}, outputs={\'Out\': out})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 41, in create_model\n    qids) = fluid.layers.read_file(pyreader)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Ids"
      arguments: "read_file_0.tmp_0"
    }
    inputs {
      parameter: "W"
      arguments: "word_embedding"
    }
    outputs {
      parameter: "Out"
      arguments: "embedding_0.tmp_0"
    }
    type: "lookup_table"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "is_sparse"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "epmap"
      type: STRINGS
    }
    attrs {
      name: "is_distributed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 403, in embedding\n    \'padding_idx\': padding_idx\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 92, in _build_model\n    is_sparse=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "padding_idx"
      type: LONG
      l: -1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "remote_prefetch"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "grad_inplace"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "table_names"
      type: STRINGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "height_sections"
      type: INTS
    }
  }
  ops {
    inputs {
      parameter: "Ids"
      arguments: "read_file_0.tmp_2"
    }
    inputs {
      parameter: "W"
      arguments: "pos_embedding"
    }
    outputs {
      parameter: "Out"
      arguments: "embedding_1.tmp_0"
    }
    type: "lookup_table"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "is_sparse"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "epmap"
      type: STRINGS
    }
    attrs {
      name: "is_distributed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 403, in embedding\n    \'padding_idx\': padding_idx\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 98, in _build_model\n    name=self._pos_emb_name, initializer=self._param_initializer))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "padding_idx"
      type: LONG
      l: -1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "remote_prefetch"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "grad_inplace"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "table_names"
      type: STRINGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "height_sections"
      type: INTS
    }
  }
  ops {
    inputs {
      parameter: "Ids"
      arguments: "read_file_0.tmp_1"
    }
    inputs {
      parameter: "W"
      arguments: "sent_embedding"
    }
    outputs {
      parameter: "Out"
      arguments: "embedding_2.tmp_0"
    }
    type: "lookup_table"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "is_sparse"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "epmap"
      type: STRINGS
    }
    attrs {
      name: "is_distributed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 403, in embedding\n    \'padding_idx\': padding_idx\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 105, in _build_model\n    name=self._sent_emb_name, initializer=self._param_initializer))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "padding_idx"
      type: LONG
      l: -1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "remote_prefetch"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "grad_inplace"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "table_names"
      type: STRINGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "height_sections"
      type: INTS
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "embedding_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "embedding_1.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_0"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 107, in _build_model\n    emb_out = emb_out + position_emb_out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "embedding_2.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 108, in _build_model\n    emb_out = emb_out + sent_emb_out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "pre_encoder_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "pre_encoder_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_1"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_0.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_0.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_0.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 111, in _build_model\n    emb_out, \'nd\', self._prepostprocess_dropout, name=\'pre_encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_0.tmp_2"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_0.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 111, in _build_model\n    emb_out, \'nd\', self._prepostprocess_dropout, name=\'pre_encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "read_file_0.tmp_3"
    }
    inputs {
      parameter: "Y"
      arguments: "read_file_0.tmp_3"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_0.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 116, in _build_model\n    x=input_mask, y=input_mask, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_0.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 119, in _build_model\n    x=self_attn_mask, scale=10000.0, bias=-1.0, bias_after_scale=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: -1.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 10000.0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
      arguments: "scale_0.tmp_0"
    }
    outputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    type: "stack"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 8825, in stack\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 121, in _build_model\n    x=[self_attn_mask] * self._n_head, axis=1)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_2.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_0.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_0.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_0.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_1.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_1.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_1.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_2.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_2.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_2.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_2.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_2.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_2.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_1.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_1.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_1.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_1.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_1.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_2"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_0.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_0.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_1.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_1.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_1.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_2.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_2.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_2.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_3.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_3.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_3.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_3.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_3.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_3.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_3.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_3.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_3.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_3.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_2.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_2.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_2.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "dropout_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_3"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_0_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_0_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_3"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_1.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_1.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_1.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_1.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_4.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_4.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_4.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_4.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_4.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_4.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_5.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_5.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_0_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_5.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_5.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_3.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_3.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_3.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_1.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_4"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_0_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_0_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_4"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_2.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_2.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_2.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_2.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_6.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_6.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_6.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_2.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_7.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_7.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_7.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_2.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_8.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_8.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_8.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_6.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_6.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_4.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_6.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_4.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_4.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_7.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_7.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_5.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_7.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_5.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_5.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_8.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_8.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_6.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_8.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_6.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_6.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_4.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_2.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_2.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_5.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_3.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_3.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_5"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_5"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_1.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_1.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_4.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_4.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_4.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_6.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_4.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_4.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_7.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_7.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_7.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_7.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_7.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_7.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_9.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_9.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_9.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_9.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_5.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_5.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_5.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_2.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_6"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_1_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_1_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_6"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_3.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_3.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_3.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_3.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_10.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_10.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_10.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_10.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_10.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_10.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_11.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_11.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_1_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_11.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_11.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_6.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_6.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_6.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_3.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_7"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_1_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_1_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_7"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_4.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_4.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_4.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_4.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_12.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_12.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_12.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_4.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_13.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_13.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_13.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_4.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_14.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_14.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_14.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_12.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_12.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_8.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_12.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_8.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_8.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_13.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_13.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_9.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_13.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_9.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_9.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_14.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_14.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_10.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_14.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_10.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_10.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_8.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_3.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_3.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_9.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_5.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_5.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_8"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_8"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_2.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_2.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_7.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_7.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_7.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_10.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_6.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_6.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_11.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_11.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_11.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_11.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_11.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_11.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_15.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_15.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_15.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_15.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_8.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_8.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_8.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_4.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_9"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_2_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_2_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_9"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_5.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_5.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_5.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_5.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_16.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_16.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_16.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_16.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_16.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_16.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_17.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_17.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_2_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_17.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_17.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_9.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_9.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_9.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_5.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_10"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_2_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_2_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_10"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_6.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_6.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_6.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_6.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_18.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_18.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_18.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_6.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_19.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_19.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_19.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_6.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_20.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_20.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_20.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_18.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_18.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_12.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_18.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_12.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_12.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_19.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_19.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_13.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_19.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_13.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_13.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_20.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_20.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_14.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_20.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_14.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_14.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_12.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_4.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_4.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_13.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_7.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_7.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_11"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_11"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_3.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_3.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_10.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_10.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_10.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_14.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_8.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_8.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_15.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_15.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_15.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_15.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_15.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_15.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_21.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_21.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_21.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_21.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_11.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_11.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_11.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_6.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_12"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_3_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_3_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_12"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_7.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_7.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_7.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_7.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_22.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_22.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_22.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_22.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_22.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_22.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_23.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_23.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_3_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_23.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_23.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_12.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_12.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_12.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_7.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_13"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_3_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_3_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_13"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_8.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_8.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_8.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_8.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_24.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_24.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_24.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_8.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_25.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_25.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_25.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_8.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_26.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_26.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_26.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_24.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_24.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_16.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_24.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_16.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_16.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_25.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_25.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_17.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_25.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_17.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_17.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_26.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_26.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_18.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_26.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_18.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_18.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_16.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_5.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_5.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_17.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_9.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_9.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_14"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_14"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_4.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_4.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_13.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_13.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_13.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_18.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_10.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_10.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_19.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_19.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_19.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_19.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_19.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_19.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_27.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_27.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_27.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_27.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_14.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_14.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_14.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_8.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_15"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_4_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_4_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_15"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_9.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_9.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_9.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_9.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_28.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_28.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_28.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_28.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_28.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_28.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_29.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_29.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_4_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_29.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_29.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_15.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_15.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_15.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_9.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_16"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_4_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_4_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_16"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_10.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_10.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_10.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_10.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_30.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_30.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_30.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_10.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_31.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_31.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_31.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_10.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_32.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_32.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_32.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_30.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_30.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_20.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_30.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_20.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_20.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_31.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_31.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_21.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_31.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_21.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_21.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_32.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_32.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_22.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_32.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_22.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_22.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_20.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_6.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_6.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_21.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_11.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_11.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_17"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_17"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_5.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_5.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_16.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_16.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_16.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_22.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_12.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_12.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_23.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_23.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_23.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_23.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_23.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_23.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_33.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_33.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_33.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_33.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_17.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_17.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_17.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_10.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_18"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_5_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_5_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_18"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_11.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_11.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_11.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_11.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_34.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_34.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_34.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_34.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_34.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_34.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_35.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_35.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_5_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_35.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_35.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_18.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_18.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_18.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_11.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_19"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_5_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_5_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_19"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_12.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_12.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_12.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_12.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_36.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_36.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_36.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_12.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_37.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_37.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_37.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_12.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_38.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_38.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_38.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_36.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_36.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_24.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_36.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_24.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_24.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_37.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_37.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_25.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_37.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_25.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_25.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_38.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_38.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_26.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_38.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_26.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_26.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_24.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_7.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_7.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_25.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_13.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_13.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_20"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_20"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_6.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_6.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_19.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_19.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_19.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_26.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_14.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_14.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_27.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_27.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_27.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_27.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_27.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_27.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_39.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_39.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_39.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_39.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_20.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_20.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_20.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_12.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_21"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_6_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_6_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_21"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_13.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_13.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_13.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_13.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_40.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_40.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_40.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_40.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_40.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_40.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_41.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_41.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_6_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_41.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_41.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_21.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_21.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_21.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_13.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_22"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_6_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_6_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_22"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_14.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_14.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_14.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_14.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_42.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_42.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_42.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_14.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_43.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_43.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_43.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_14.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_44.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_44.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_44.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_42.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_42.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_28.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_42.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_28.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_28.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_43.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_43.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_29.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_43.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_29.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_29.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_44.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_44.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_30.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_44.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_30.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_30.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_28.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_8.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_8.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_29.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_15.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_15.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_23"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_23"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_7.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_7.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_22.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_22.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_22.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_30.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_16.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_16.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_31.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_31.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_31.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_31.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_31.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_31.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_45.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_45.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_45.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_45.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_23.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_23.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_23.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_14.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_24"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_7_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_7_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_24"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_15.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_15.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_15.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_15.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_46.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_46.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_46.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_46.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_46.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_46.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_47.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_47.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_7_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_47.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_47.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_24.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_24.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_24.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_15.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_25"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_7_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_7_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_25"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_16.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_16.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_16.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_16.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_48.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_48.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_48.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_16.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_49.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_49.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_49.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_16.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_50.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_50.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_50.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_48.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_48.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_32.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_48.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_32.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_32.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_49.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_49.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_33.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_49.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_33.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_33.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_50.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_50.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_34.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_50.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_34.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_34.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_32.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_9.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_9.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_33.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_17.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_17.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_26"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_26"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_8.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_8.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_25.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_25.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_25.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_34.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_18.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_18.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_35.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_35.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_35.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_35.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_35.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_35.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_51.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_51.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_51.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_51.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_26.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_26.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_26.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_16.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_27"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_8_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_8_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_27"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_17.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_17.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_17.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_17.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_52.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_52.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_52.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_52.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_52.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_52.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_53.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_53.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_8_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_53.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_53.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_27.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_27.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_27.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_17.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_28"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_8_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_8_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_28"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_18.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_18.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_18.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_18.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_54.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_54.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_54.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_18.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_55.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_55.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_55.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_18.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_56.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_56.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_56.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_54.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_54.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_36.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_54.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_36.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_36.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_55.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_55.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_37.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_55.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_37.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_37.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_56.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_56.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_38.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_56.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_38.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_38.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_36.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_10.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_10.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_37.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_19.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_19.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_29"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_29"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_9.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_9.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_28.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_28.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_28.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_38.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_20.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_20.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_39.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_39.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_39.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_39.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_39.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_39.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_57.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_57.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_57.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_57.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_29.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_29.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_29.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_18.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_30"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_9_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_9_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_30"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_19.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_19.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_19.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_19.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_58.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_58.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_58.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_58.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_58.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_58.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_59.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_59.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_9_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_59.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_59.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_30.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_30.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_30.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_19.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_31"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_9_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_9_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_31"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_20.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_20.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_20.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_20.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_60.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_60.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_60.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_20.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_61.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_61.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_61.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_20.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_62.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_62.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_62.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_60.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_60.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_40.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_60.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_40.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_40.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_61.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_61.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_41.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_61.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_41.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_41.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_62.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_62.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_42.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_62.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_42.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_42.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_40.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_11.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_11.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_41.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_21.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_21.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_32"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_32"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_10.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_10.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_31.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_31.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_31.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_42.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_22.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_22.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_43.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_43.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_43.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_43.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_43.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_43.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_63.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_63.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_63.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_63.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_32.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_32.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_32.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_20.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_33"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_10_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_10_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_33"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_21.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_21.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_21.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_21.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_64.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_64.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_64.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_64.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_64.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_64.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_65.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_65.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_10_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_65.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_65.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_33.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_33.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_33.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_21.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_34"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_10_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_10_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_34"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_22.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_22.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_22.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_22.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_query_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_66.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_66.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_query_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_66.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 60, in __compute_qkv\n    bias_attr=name + \'_query_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_22.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_key_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_67.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_67.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_key_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_67.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 67, in __compute_qkv\n    bias_attr=name + \'_key_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_22.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_value_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_68.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_68.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_value_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_68.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 74, in __compute_qkv\n    bias_attr=name + \'_value_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 129, in multi_head_attention\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_66.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_66.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_44.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_66.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_44.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_44.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 142, in multi_head_attention\n    q = __split_heads(q, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_67.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_67.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_45.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_67.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_45.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_45.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 143, in multi_head_attention\n    k = __split_heads(k, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "fc_68.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_68.tmp_1"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_46.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 88, in __split_heads\n    x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 12
      ints: 64
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_68.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_46.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_46.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 92, in __split_heads\n    return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 144, in multi_head_attention\n    v = __split_heads(v, n_head)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_44.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "scale_12.tmp_0"
    }
    type: "scale"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9267, in scale\n    \'bias_after_scale\': bias_after_scale\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 115, in scaled_dot_product_attention\n    scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 0.125
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "scale_12.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_45.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_23.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 116, in scaled_dot_product_attention\n    product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_23.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "stack_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_35"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 118, in scaled_dot_product_attention\n    product += attn_bias\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tmp_35"
    }
    outputs {
      parameter: "Out"
      arguments: "softmax_11.tmp_0"
    }
    type: "softmax"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1885, in softmax\n    \"use_cudnn\": use_cudnn})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 119, in scaled_dot_product_attention\n    weights = layers.softmax(product)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_11.tmp_0"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_34.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_34.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 125, in scaled_dot_product_attention\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_34.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "transpose_46.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "matmul_24.tmp_0"
    }
    type: "matmul"
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "alpha"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 4937, in matmul\n    \'alpha\': float(alpha),\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 126, in scaled_dot_product_attention\n    out = layers.matmul(weights, v)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 147, in multi_head_attention\n    dropout_rate)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "transpose_Y"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "transpose_X"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "matmul_24.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_47.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "transpose_47.tmp_1"
    }
    type: "transpose2"
    attrs {
      name: "data_format"
      type: STRING
      s: "AnyLayout"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5708, in transpose\n    attrs={\'axis\': perm})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 103, in __combine_heads\n    trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INTS
      ints: 0
      ints: 2
      ints: 1
      ints: 3
    }
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "X"
      arguments: "transpose_47.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "transpose_47.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_47.tmp_0"
    }
    type: "reshape2"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6424, in reshape\n    \"XShape\": x_shape})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 109, in __combine_heads\n    inplace=True)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 149, in multi_head_attention\n    out = __combine_heads(ctx_multiheads)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 0
      ints: 0
      ints: 768
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "transpose_47.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_output_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_69.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_69.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_multi_head_att_output_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_69.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 158, in multi_head_attention\n    bias_attr=name + \'_output_fc.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 273, in encoder_layer\n    name=name + \'_multi_head_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_69.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_35.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_35.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_35.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_22.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_36"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_11_post_att_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_11_post_att_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_36"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_23.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_23.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_23.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 279, in encoder_layer\n    name=name + \'_post_att\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "layer_norm_23.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_ffn_fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_70.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_70.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_ffn_fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_70.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_70.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_70.tmp_2"
    }
    type: "relu"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 181, in positionwise_feed_forward\n    bias_attr=name + \'_fc_0.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_70.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_ffn_fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_71.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_71.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "encoder_layer_11_ffn_fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_71.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 193, in positionwise_feed_forward\n    bias_attr=name + \'_fc_1.b_0\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 291, in encoder_layer\n    name=name + \'_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 2
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_71.tmp_1"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_36.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_36.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 229, in pre_post_process_layer\n    is_test=False)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_36.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "layer_norm_23.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_37"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/math_op_patch.py\", line 144, in __impl__\n    attrs={\'axis\': axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 207, in pre_post_process_layer\n    out = out + prev_out if prev_out else out\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "encoder_layer_11_post_ffn_layer_norm_bias"
    }
    inputs {
      parameter: "Scale"
      arguments: "encoder_layer_11_post_ffn_layer_norm_scale"
    }
    inputs {
      parameter: "X"
      arguments: "tmp_37"
    }
    outputs {
      parameter: "Mean"
      arguments: "layer_norm_24.tmp_0"
    }
    outputs {
      parameter: "Variance"
      arguments: "layer_norm_24.tmp_1"
    }
    outputs {
      parameter: "Y"
      arguments: "layer_norm_24.tmp_2"
    }
    type: "layer_norm"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3331, in layer_norm\n    \"begin_norm_axis\": begin_norm_axis})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 220, in pre_post_process_layer\n    initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 297, in encoder_layer\n    name=name + \'_post_ffn\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/transformer_encoder.py\", line 336, in encoder\n    name=name + \'_layer_\' + str(i))\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 140, in _build_model\n    name=\'encoder\')\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 82, in __init__\n    self._build_model(src_ids, position_ids, sentence_ids, input_mask)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 49, in create_model\n    use_fp16=args.use_fp16)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "begin_norm_axis"
      type: INT
      i: 2
    }
    attrs {
      name: "epsilon"
      type: FLOAT
      f: 9.999999747378752e-06
    }
  }
  ops {
    inputs {
      parameter: "Input"
      arguments: "layer_norm_24.tmp_2"
    }
    outputs {
      parameter: "Out"
      arguments: "slice_0.tmp_0"
    }
    type: "slice"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9174, in slice\n    \'ends\': ends})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 148, in get_pooled_output\n    input=self._enc_out, axes=[1], starts=[0], ends=[1])\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 51, in create_model\n    cls_feats = ernie.get_pooled_output()\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "ends"
      type: INTS
      ints: 1
    }
    attrs {
      name: "starts"
      type: INTS
      ints: 0
    }
    attrs {
      name: "axes"
      type: INTS
      ints: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "slice_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "pooled_fc.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_72.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 155, in get_pooled_output\n    bias_attr=\"pooled_fc.b_0\")\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 51, in create_model\n    cls_feats = ernie.get_pooled_output()\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_72.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "pooled_fc.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_72.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 155, in get_pooled_output\n    bias_attr=\"pooled_fc.b_0\")\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 51, in create_model\n    cls_feats = ernie.get_pooled_output()\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_72.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_72.tmp_2"
    }
    type: "tanh"
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 338, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/model/ernie.py\", line 155, in get_pooled_output\n    bias_attr=\"pooled_fc.b_0\")\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 51, in create_model\n    cls_feats = ernie.get_pooled_output()\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_72.tmp_2"
    }
    outputs {
      parameter: "Mask"
      arguments: "dropout_37.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "dropout_37.tmp_0"
    }
    type: "dropout"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 1397, in dropout\n    \'dropout_implementation\': dropout_implementation,\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 55, in create_model\n    dropout_implementation=\"upscale_in_train\")\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "dropout_implementation"
      type: STRING
      s: "upscale_in_train"
    }
    attrs {
      name: "seed"
      type: INT
      i: 0
    }
    attrs {
      name: "fix_seed"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "is_test"
      type: INT
      i: 1
    }
    attrs {
      name: "dropout_prob"
      type: FLOAT
      f: 0.10000000149011612
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "dropout_37.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "cls_out_w"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_73.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 323, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 63, in create_model\n    name=\"cls_out_b\", initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_73.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "cls_out_b"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_73.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 336, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 63, in create_model\n    name=\"cls_out_b\", initializer=fluid.initializer.Constant(0.)))\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Label"
      arguments: "read_file_0.tmp_4"
    }
    inputs {
      parameter: "Logits"
      arguments: "fc_73.tmp_1"
    }
    outputs {
      parameter: "Loss"
      arguments: "softmax_with_cross_entropy_0.tmp_1"
    }
    outputs {
      parameter: "Softmax"
      arguments: "softmax_with_cross_entropy_0.tmp_0"
    }
    type: "softmax_with_cross_entropy"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 6057, in softmax_with_cross_entropy\n    \'numeric_stable_mode\': numeric_stable_mode\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 73, in create_model\n    logits=logits, label=labels, return_softmax=True)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "ignore_index"
      type: INT
      i: -100
    }
    attrs {
      name: "numeric_stable_mode"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "soft_label"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "softmax_with_cross_entropy_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "mean_0.tmp_0"
    }
    type: "mean"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 9560, in mean\n    type=\"mean\", inputs={\"X\": x}, attrs={}, outputs={\"Out\": out})\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 74, in create_model\n    loss = fluid.layers.mean(x=ce_loss)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "K"
    }
    inputs {
      parameter: "X"
      arguments: "softmax_with_cross_entropy_0.tmp_0"
    }
    outputs {
      parameter: "Indices"
      arguments: "top_k_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "top_k_0.tmp_0"
    }
    type: "top_k"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 5012, in topk\n    attrs=attrs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/metric_op.py\", line 60, in accuracy\n    topk_out, topk_indices = nn.topk(input, k=k)\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 80, in create_model\n    accuracy = fluid.layers.accuracy(input=probs, label=labels, total=num_seqs)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "k"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Indices"
      arguments: "top_k_0.tmp_1"
    }
    inputs {
      parameter: "Label"
      arguments: "read_file_0.tmp_4"
    }
    inputs {
      parameter: "Out"
      arguments: "top_k_0.tmp_0"
    }
    outputs {
      parameter: "Accuracy"
      arguments: "accuracy_0.tmp_0"
    }
    outputs {
      parameter: "Correct"
      arguments: "accuracy_0.tmp_1"
    }
    outputs {
      parameter: "Total"
      arguments: "create_tensor_0"
    }
    type: "accuracy"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 1654, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/ibdo/.local/lib/python3.6/site-packages/paddle/fluid/layers/metric_op.py\", line 76, in accuracy\n    \"Total\": [total],\n"
      strings: "  File \"/home/ibdo/Desktop/nlp111/ERNIE/finetune/classifier.py\", line 80, in create_model\n    accuracy = fluid.layers.accuracy(input=probs, label=labels, total=num_seqs)\n"
      strings: "  File \"run_classifier.py\", line 134, in main\n    ernie_config=ernie_config)\n"
      strings: "  File \"run_classifier.py\", line 281, in <module>\n    main(args)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
}
version {
  version: 0
}

